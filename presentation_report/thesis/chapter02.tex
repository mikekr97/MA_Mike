

% LaTeX file for Chapter 02



\chapter{Methods}

This chapter introduces the methodological foundations for the experiments used in this thesis. Section~\ref{sec:methods_tram_dags} presents the concept and functionality of TRAM-DAGs, along with the necessary theoretical background. Section~\ref{sec:methods_ite} provides the framework for estimating individualized treatment effects.  These methods form the basis for the experiments and analyses that follow in Chapter~\ref{ch:experiments}.

% Sections~\ref{sec:methods_experiment1}--\ref{sec:methods_experiment4} describe the setup of the four experiments conducted to address the research questions outlined in Section~\ref{sec:goals_contributions}.


\section{TRAM-DAGs} \label{sec:methods_tram_dags}

The goal of TRAM-DAGs \citep{sick2025} is to estimate the structural equations of a given DAG in a flexible and, if desired, interpretable way. This enables the sampling of observational and interventional distributions, as well as to make counterfactual statements. The approach requires data and a known causal graph describing the underlying structure. It is assumed that there are no hidden confounders. TRAM-DAGs estimate, for each variable $X_i$, a transformation function $Z_i = h_i(X_i \mid \operatorname{pa}(X_i))$, where $Z_i$ denotes the noise variable and $\operatorname{pa}(X_i)$ are the causal parents of $X_i$. Crucially, for continuous variables, this relationship can be inverted to yield the structural equation $X_i = h_i^{-1}(Z_i \mid \operatorname{pa}(X_i))$. The monotonically increasing transformation functions $h_i$ allow to determine the conditional distribution  $X_i \mid \operatorname{pa}(X_i)$ on a latent scale $Z_i$. They are based on the framework of transformation models as introduced by \citet{hothorn2014} that were extended to deep TRAMs by \citet{sick2020}. The following summarizes the key ideas of these models, which form the building blocks of TRAM-DAGs.


\subsection{Transformation Models} \label{sec:trams}


% Transformation models are a flexible class of distributional regression models applicable to various data types. They can be regarded as generalizations of standard models such as linear regression, logistic regression, or proportional odds models, while also allowing for the modeling of outcome distributions that do not belong to a known parametric family. This is achieved by modeling components of the transformation function in a flexible, semi-parametric way, thereby reducing the strength of assumptions required about the underlying data-generating process. The basic form of transformation models can be described by 

Transformation models are a flexible class of distributional regression models that can be applied to various data types. A linear transformation model is given by

\begin{equation}
F_{Y \mid \mathbf{X}=\mathbf{x}}(y) = F_Z(h(y \mid \mathbf{x})) =  F_Z(h_I(y) - \mathbf{x}^\top \boldsymbol{\beta})
\label{eq:transformation_model}
\end{equation}

where $F_{Y \mid \mathbf{X}=\mathbf{x}}(y)$ is the conditional cumulative distribution function (CDF) of the outcome variable $Y$ given the predictors $\mathbf{X}=\mathbf{x}$. The transformation function $h(y \mid \mathbf{x})$ maps the outcome variable $y$ onto the latent scale of variable $Z$, and $F_Z$ is the CDF of $Z$, the so-called inverse-link function that maps $h(y \mid \mathbf{x})$ to probabilities. In its simplest form, as shown in Equation~\ref{eq:transformation_model}, the transformation function can be split into an intercept part $h_I(y)$ and a linear shift component $\mathbf{x}^\top \boldsymbol{\beta}$, where $\mathbf{x}$ are the predictors and $\boldsymbol{\beta}$ the corresponding coefficients.

If the latent distribution $Z$ is chosen to be the standard logistic, each coefficient $\beta_i$ can be interpreted as log-odds ratio: an increase of one unit in the predictor $x_i$, while holding other predictors unchanged, increases the log-odds (latent scale) of the outcome $Y$ by $\beta_i$. After applying the inverse-link function $F_Z$, this can induce a non-linear change to the conditional distribution of $Y$ on the original scale. Further discussion on the choice of latent distribution and the interpretation of coefficients is provided in Appendix~\ref{sec:interpretation_linear_coefficients}.

For a continuous outcome $Y$, the intercept $h_I(y)$ is represented by a Bernstein polynomial, which is a flexible and monotonically increasing function:

\begin{equation}
h_I(y) = \frac{1}{M + 1} \sum_{k=0}^{M} \vartheta_k \, \text{B}_{k, M}(y),
\label{eq:methods_bernstein_polynomial}
\end{equation}

where $\vartheta_k, k = 0, \ldots, M$ are the coefficients and $\text{B}_{k, M}(y)$ are the corresponding Bernstein basis polynomials. The coefficients $\vartheta_k$ are constrained to be monotonically increasing to ensure that the transformation function $h_I(y)$ is strictly increasing. More details on the technical implementation of the Bernstein polynomial (Equation~\ref{eq:methods_bernstein_polynomial}) in the context of deep-TRAMs are provided in the Appendix~\ref{sec:bernstein_polynomial}.






For a discrete outcome $Y$, the intercept $h_I$ is represented by a set of cut-points $\vartheta_k$, which define the thresholds separating the different levels of the outcome. For example, a binary outcome requires one cut-point, while an ordinal outcome with $K$ levels requires $K-1$ cut-points. The transformation model is given by:

\begin{equation}
P(Y \leq y_k \mid \mathbf{X} = \mathbf{x}) = F_Z(\vartheta_k + \mathbf{x}^\top \boldsymbol{\beta}), \quad k = 1, 2, \ldots, K - 1.
\label{eq:discrete_tram}
\end{equation}


A visual representation for a continuous and discrete (ordinal) outcome is shown in Figure~\ref{fig:tram_cont_ord}.


% include image /img/tram_cont_ord.png
\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{img/tram_cont_ord.png}
\caption{Left: Example of a transformation model for a continuous outcome $Y$ (Equation~\ref{eq:transformation_model}) with a smooth transformation function. Right: Example of a transformation model for an ordinal outcome $Y$ (Equation~\ref{eq:discrete_tram}) with 4 levels. The transformation function consists of cut-points separating the levels of the outcome.
In both cases the latent distribution $Z$ is the standard logistic and the predictors $\mathbf{x}$ induce a linear (vertical) shift of the transformation function. Plots adapted from visualizations similar to those used in \citet{sick2025}.}
\label{fig:tram_cont_ord}
\end{figure}


The parameters $\boldsymbol{\beta}$ and $\boldsymbol{\vartheta}$ are estimated by minimizing the negative log-likelihood (NLL), defined as:


\begin{equation}
\text{NLL} = - \frac{1}{n} \sum_{i=1}^{n} \log \left(f_{Y \mid \mathbf{X} = \mathbf{x}_i}(y_i)\right),
\label{eq:nll_tram}
\end{equation}

where $f_{Y \mid \mathbf{X} = \mathbf{x}_i}(y_i)$ is the conditional density function of the outcome $Y$ given the predictors $\mathbf{x}$ of the $i$-th observation under the current parameterization. A full derivation is provided in Appendix \ref{sec:nll}.

Throughout this thesis, these transformation models form the foundation for estimating conditional distributions of variables within the TRAM-DAG framework. Unless stated otherwise, the latent distribution $F_Z$ is chosen to be the standard logistic, resulting in a logistic transformation model.



\subsection{Deep TRAMs} \label{sec:deep_trams}

The transformation models introduced in Section~\ref{sec:trams} were extended into deep TRAMs using a modular neural network architecture \citep{sick2020}. The goal is to obtain a parameterized transformation function of the form

\begin{equation}
h(y \mid \mathbf{x}_L, \mathbf{x}_C ) = h_I(y) + \mathbf{x}_L^\top \boldsymbol{\beta} + f(\mathbf{x}_C),
\label{eq:deep_tram_old}
\end{equation}

where $h_I(y)$ is the intercept (simple or complex), $\mathbf{x}_{L}$ are predictors with a linear effect on the transformation function, and $\mathbf{x}_{C}$ are those with a potentially complex, non-linear influence. The parameters for each component of the model -- intercept, linear shift, and complex shift -- are obtained by a neural network module. The user can assign predictors to these components depending on the assumed structure of the data and the desired flexibility. Figure~\ref{fig:deep_tram} illustrates an example of a transformation function (Equation~\ref{eq:deep_tram_old}) with these three components (SI-LS-CS). For simplicity, it shows the simple intercept (SI) case where the intercept does not depend on predictors.


\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{img/deep_tram.png}
\caption{Modular deep transformation model (deep TRAM). The transformation function $h(y \mid \mathbf{x})$ (Equation~\ref{eq:deep_tram_old}) is constructed from the outputs of three neural networks: intercept (SI/CI), linear shift (LS), and complex shift (CS). The intercept module (left) shows the simple intercept (SI) case and could be extended to a complex intercept (CI) by feeding predictors into the module and adding hidden layers with nonlinear activation functions.}
\label{fig:deep_tram}
\end{figure}

\medskip


\textbf{Intercept:} The intercept defines the baseline shape of the transformation function when $\mathbf{x}_{L}^\top \boldsymbol{\beta} = 0$ and $f(\mathbf{x}_{C}) = 0$. For continuous outcomes, it is modeled using a smooth Bernstein polynomial (Equation~\ref{eq:methods_bernstein_polynomial}), and for discrete outcomes via cut-points. The preliminary parameters $\tilde{\vartheta}_k$ are the output nodes of a neural network, and subsequently transformed into monotonically increasing parameters $\vartheta_k$. A simple intercept (SI) assumes no dependency on predictors and uses only a constant input. A complex intercept (CI), by contrast, allows the parameters to vary as a function of selected predictors by feeding $\mathbf{x}$ into a neural network with hidden layers, producing predictor-dependent parameters $\vartheta_k(\mathbf{x})$. This allows the baseline distribution to adapt flexibly to different covariate settings. Details on the computation of the Bernstein polynomial (Equation~\ref{eq:methods_bernstein_polynomial}) are provided in Appendix~\ref{sec:bernstein_polynomial}.

\medskip

\textbf{Linear shift:} Predictors with an assumed linear influence on the transformation function are modeled via the linear shift (LS) component. Here, a neural network without hidden layers and without bias nodes receives $\mathbf{x}_{L}$ as input and returns a linear combination $\mathbf{x}_{L}^\top \boldsymbol{\beta}$ as output. This results in a vertical, linear shift of the transformation function. The parameters $\boldsymbol{\beta}$ are interpretable coefficients, and in the case of a logistic transformation model, represent log-odds ratios. See Appendix~\ref{sec:interpretation_linear_coefficients} for more details.

\medskip

\textbf{Complex shift:} Non-linear dependencies between predictors and the transformation function can be modeled by the complex shift (CS) component. The corresponding predictors $\mathbf{x}_{C}$ are input into a deep neural network (with one or more hidden layers and non-linear activation functions), yielding a scalar output $f(\mathbf{x}_{C})$. This allows modeling of non-linear predictor effects, including interactions between predictors if multiple predictors are input into the neural network module. See Appendix~\ref{sec:complex_shift} for an example of modeling an interaction with a complex shift for the purpose of estimating individualized treatment effects (ITEs).

\medskip

% \textbf{Level of complexity:} A key advantage of the deep TRAM architecture is that users can specify the role of each predictor: linear (LS), complex (CS), or influencing the shape of the transformation function (CI). For example, \citet{herzog2023} predicted the ordinal functional outcome three months after stroke by combining structured tabular data with image features (via a CNN) as predictors. This flexible design allows the integration of different data modalities into a single model.

\textbf{Level of complexity:} A key advantage of the deep TRAM architecture is that users can specify the role of each predictor: linear (LS), complex (CS), or influencing the shape of the transformation function (CI). For example, \citet{kook2022} demonstrated the use of deep transformation models with flexible components for ordinal outcomes and multi-modal data. \citet{herzog2023} later applied this framework in the medical domain, predicting ordinal stroke outcomes by combining structured tabular data with image features (via a CNN). This design enables the integration of different data modalities into a single model.



\medskip

The resulting distribution function of deep TRAMs is invariant to the choice of the inverse-link function $F_Z$ (latent distribution) in an unconditional \citep{hothorn2018} or fully flexible (CI) setting. However, once restrictions are introduced on the influence of the predictors -- such as LS or CS -- the model assumes a fixed scale of dependence. The choice of latent distribution may depend on (i) assumptions about the data-generating process, (ii) the conventional and widely used interpretation scale for parameters (e.g., log-odds ratios or log-hazard ratios, see Appendix~\ref{sec:interpretation_linear_coefficients}), and (iii) a data-driven criterion such as selecting the distribution that minimizes the negative log-likelihood (NLL) on the observed data.

%
\medskip

\textbf{Parameter estimation:} The weights of the neural networks are learned by minimizing the NLL (Equation~\ref{eq:nll_tram}) of the conditional model. Starting from random weight initialization, the networks are trained using the Adam optimizer \citep{kingma2015}. Outputs of the modules are assembled to compute the NLL, and backpropagation \citep{rumelhart1986} is used to adjust weights iteratively. Deep learning techniques such as dropout \citep{srivastava2014}, early stopping \citep{prechelt2012}, and batch normalization \citep{Ioffe2015} can be applied to prevent overfitting and improve generalization. Nonlinear activation functions (e.g., ReLU \citealp{glorot2011} or sigmoid \citealp{rumelhart1986}) are used in hidden layers to capture complex relationships. \citet{schmidhuber2015} provides an extensive historical overview of deep learning methods and their development.




\subsection{TRAM-DAGs: Deep TRAMS applied in a causal setting} \label{sec:tram_dags}



In TRAM-DAGs, deep transformation models (see Section~\ref{sec:deep_trams}) are applied within a causal setting. We assume a pre-specified DAG that represents the causal dependencies among variables. 
Figure~\ref{fig:tram_dag} illustrates the basic idea using a simple DAG with three variables and no hidden confounders. Each node's conditional distribution is modeled using a deep TRAM (Equation~\ref{eq:deep_tram_old}), given its parent variables in the DAG. The assumed influence of the parent variables must be specified as CI, LS or CS. In this example, $X_1$ is a continuous source node (i.e., without parents) and its transformation function consists only of a simple intercept (SI). $X_2$ is also continuous and depends on $X_1$ by a linear shift (LS). $X_3$ is an ordinal variable with four levels; its transformation function is influenced linearly by $X_1$ (LS) and non-linearly by $X_2$ (CS). The cut-points $h(x_{3,k} \mid x_1, x_2)$ represent the cumulative log-odds of levels $k = 1, 2, 3$ of $X_3$. The probability of the highest category ($k = 4$) is the complement of the cumulative probability of the first three.


% include image /img/tram_dag.png
\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{img/tram_dag.png}
\caption{Example of a TRAM-DAG with three variables: $X_1$, $X_2$, and $X_3$. Each variable's distribution is modeled conditionally on its parents using deep TRAMs. Arrows indicate the causal dependencies. Plots adapted from visualizations similar to those used in \citet{sick2025}.}
\label{fig:tram_dag}
\end{figure}

The DAG in Figure~\ref{fig:tram_dag}, along with its assumed dependencies, can be represented by a meta-adjacency matrix (Equation~\ref{eq:MA}), where rows indicate sources and columns indicate targets of causal influence:


\begin{equation}
\mathbf{MA} =
\begin{bmatrix}
  0 & \text{LS} & \text{LS} \\
  0 & 0  & \text{CS} \\
  0 & 0  & 0
\end{bmatrix}
\label{eq:MA}
\end{equation}

Based on this DAG, each variable's conditional distribution is modeled using a deep TRAM, enabling generative sampling and causal inference. 

% The corresponding conditional distributions are:
% 
% \[
% \begin{aligned}
% X_1 &\sim F_Z(h_I(x_1)) \\
% X_2 &\sim F_Z(h_I(x_2) + \mathrm{LS}_{x_1}) \\
% X_3 &\sim F_Z(h_I(x_3) + \mathrm{LS}_{x_1} + \mathrm{CS}_{x_2})
% \end{aligned}
% \]

\medskip 

\textbf{Constructing the modular neural network:} As described in Section~\ref{sec:deep_trams}, the transformation functions are implemented using a modular neural network. The network takes as input the observed variables $\mathbf{x}$ and the meta-adjacency matrix (Equation~\ref{eq:MA}), which encodes the assumed causal structure. This matrix acts as a binary mask that restricts information flow to follow the directed edges of the graph, ensuring that only causal parent-to-child relationships are modeled. As a result, spurious dependencies such as $X_2 \rightarrow X_1$ are prevented. This masking approach is inspired by the Masked Autoregressive Flow (MAF) framework \citep{papamakarios2017}, where connectivity within the network is masked to control which inputs each output can depend on.

Before training the model, the input data must be appropriately pre-processed. Discrete variables with few categories are dummy encoded, and continuous variables should be scaled prior to input. Further discussion of encoding and scaling is provided in Appendix~\ref{sec:encoding_discrete_variables} and~\ref{sec:scaling_continuous_variables}.

Once the data are preprocessed and the structure is specified, the architecture of the neural networks for the complex shift or complex intercept must be defined -- including choices such as depth, width, activation functions, and whether to use dropout or batch normalization. These decisions depend on the assumed complexity of the effects and the need to regularize against overfitting.

Each node's transformation function (Equation~\ref{eq:deep_tram_old}) is assembled from the outputs of its three modular components: the intercept (SI or CI), linear shift (LS), and complex shift (CS). Model training is performed by minimizing the NLL (Equation~\ref{eq:nll_tram}), optimizing all parameters at the same time. The estimated parameters $\boldsymbol{\beta}$ in the linear shift are interpretable as log-odds ratios corresponding to a one-unit increase in the respective parent variable, holding all other parent variables unchanged.




\subsection{Sampling from TRAM-DAGs} \label{methods:sampling}

Once a TRAM-DAG is fitted on data, it can be used to sample from the observational or interventional distribution, or to perform counterfactual queries. For continuous outcomes, the structural equations $X_i = f(Z_i, \text{pa}(X_i))$ are represented by the inverse of the conditional transformation functions, i.e., $X_i = h^{-1}(Z_i \mid \text{pa}(X_i))$, since the transformation function maps from observed values to the latent scale: $Z_i = h(X_i \mid \text{pa}(X_i))$.


\medskip

\textbf{Observational sampling:} The sampling process from the observational distribution is described in Algorithm~\ref{alg:sampling} and illustrated in Figure~\ref{fig:sampling}. In each iteration, one complete sample of all variables in the DAG is generated -- i.e., a sample from the joint observational distribution.

\begin{algorithm}
\caption{Generate a complete sample from the observational distribution}
\label{alg:sampling}
\begin{algorithmic}
\State \textbf{Given:} A fitted TRAM-DAG with estimated transformation functions $Z_i = h(X_i \mid \text{pa}(X_i))$
\For{each node $X_i$ in topological order}
  \State Sample latent value $z_i \sim F_{Z_i}$ \Comment{e.g., \texttt{rlogis()} in R}
  \If{$X_i$ is continuous}
    \State Solve $h(x_i \mid \text{pa}(x_i)) - z_i = 0$ for $x_i$ \Comment{numerical root-finding}
  \ElsIf{$X_i$ is discrete}
    \State Find category $x_i = \max \left( \{0\} \cup \left\{ x : z_i > h(x \mid \text{pa}(x_i)) \right\} \right) + 1
$
  \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}


\begin{figure}[H]  % before [H]
\centering
\includegraphics[width=1\textwidth]{img/sampling.png}
\caption{One sampling iteration for the three variables from the estimated transformation functions $h(x_i \mid \text{pa}(x_i))$. The latent values $z_i$ are sampled from the standard logistic distribution. The values $x_i$ are determined by applying the inverse of the transformation function for continuous variables or by finding the corresponding category for the ordinal variable. See Algorithm~\ref{alg:sampling} for details. Plots adapted from visualizations similar to those used in \citet{sick2025}.}
\label{fig:sampling}
\end{figure}


% \textbf{Interventional sampling:} To sample from the interventional distribution, we can apply the do-operator as described by \citet{pearl1995} (though, Pearl named it set instead of do). The do-operator fixes a variable at a certain value. Hence, one can sample from the distribution of the other variables while keeping the fixed variable constant. For example, if one wants to intervene on $X_2$ and set it to a specific value $\alpha$, $\textcolor{red}{\text{do}(x_2 = \alpha})$
% and then sample from the interventional-distribution
% \[
% x_3 = \min \left\{ x : z_3 \le h(x \mid x_1, \textcolor{red}{x_2 = \alpha}) \right\}
% \]
% 
% with the same process as for the observational sampling, with the only difference that the intervened variable $X_2$ stays constant.


%  do-operator (originally referred to as "set")

\textbf{Interventional sampling:} To sample from an interventional distribution, we apply the \textit{do}-operator as described by \citet{pearl1995}. The \textit{do}-operator fixes a variable at a specific value, thereby removing causal dependencies on its (former) parents. This allows us to simulate interventions by deleting all edges pointing into the intervened variable, while sampling the remaining variables as described earlier in Algorithm~\ref{alg:sampling}. 

% For example, suppose we intervene on $X_2$ and set it to a specific value $\alpha$, denoted as $\text{do}(X_2 = \alpha)$. To sample from the resulting interventional distribution, we proceed as in observational sampling, with the only difference that $X_2$ is no longer sampled but fixed to the value $\alpha$. $X_1$ would not be affected by the intervention, as it is not a descendant of $X_2$ in the DAG. Therefore, sampling $X_3$ under this intervention would follow:
% 
% \[
% x_3 = \max \left( \{0\} \cup \left\{ x : z_3 > h(x \mid x_1, \textcolor{red}{x_2 = \alpha}) \right\} \right) + 1.
% \]

\medskip

\textbf{Counterfactual queries:} In a counterfactual query, we are interested in what the value of a variable $X_i$ would have been, had another variable $X_j$ taken a different value than actually observed. \citet{pearl_book2009} describes a three-step procedure to answer such queries. In short, let $\mathbf{x}$ denote the observed values (a sample) of all variables in a DAG and let $\mathbf{z}$ denote the corresponding latent variables inferred via the transformation functions: $z_k = h_k(x_k \mid \text{pa}(x_k))$. Then, in a counterfactual query where $X_j$ is set to a new value $\alpha$ (intervention), we use the same latent values $\mathbf{z}$, but apply the transformation functions in a post-interventional DAG with $X_j := \alpha$ to compute the counterfactual outcomes for the remaining variables.
The counterfactual estimation procedure is outlined in Algorithm~\ref{alg:single_cf} and visualized in Figure~\ref{fig:cf_viz}. Note that this visualization is simplified to illustrate only one parent variable ($X_j$); in practice, the set of parents $\text{pa}(X_i)$ may include additional predictors.

\begin{algorithm}[!ht]
\caption{Answer a single counterfactual query}
\label{alg:single_cf}
\begin{algorithmic}
\State \textbf{Given:} A TRAM-DAG model with estimated transformation functions $Z_k = h_k(X_k \mid \text{pa}(X_k))$
\State \textbf{Input:} Observed sample $\mathbf{x}$, intervention $X_j := \alpha$
\vspace{0.3em}
\State \textbf{Step 1 (Abduction):} For each observed variable $x_k$, compute the corresponding latent value $z_k = h_k(x_k \mid \text{pa}(x_k))$
\vspace{0.3em}
\State \textbf{Step 2 (Action):} Modify the DAG by setting $X_j := \alpha$ (intervention)
\vspace{0.3em}
\State \textbf{Step 3 (Prediction):} Using the counterfactual DAG and the intervened value $X_j = \alpha$, go along the causal order and determine the counterfactual values for all descendants $X_k$ of $X_j$. This is done by evaluating
\[
x_k = h_k^{-1}(z_k \mid \text{pa}^*(x_k)),
\]
where $z_k$ is the latent value obtained in Step 1, and $\text{pa}^*(x_k)$ are the (possibly updated) parent values after the intervention.
\end{algorithmic}
\end{algorithm}

\begin{figure}[!ht]
\centering
\includegraphics[width=0.65\textwidth]{img/counterfactuals.png}
\caption{Illustration of a counterfactual query for variable $X_i$, had its parent $X_j$ taken a different value, following the three-step procedure: (1) abduction of the latent value $z_i$ from the observed outcome, (2) intervention by setting $X_j = \alpha$, (3) prediction of the counterfactual outcome using the same $z_i$ and the modified transformation function. This simplified example assumes a single parent variable; in practice, $\text{pa}(X_i)$ may include multiple predictors. Plot adapted from visualizations similar to those used in \citet{sick2025}.}
\label{fig:cf_viz}
\end{figure}



While interventional distributions (e.g., $P(X_i = x_i \mid \text{do}(X_j = \alpha), z_i)$) are well-defined in both continuous and discrete settings, determining the counterfactual realizations is generally only possible for continuous variables. In the discrete case, no unique latent value $z_k$ can be recovered from an observed category, as multiple $z_k$ values may map to the same outcome. 
% Consequently, only the probabilities of counterfactual outcomes can be evaluated for discrete variables, not their exact values. This is a fundamental limitation of counterfactual reasoning in discrete settings with continuous latent variables.

% 
% \textbf{Counterfactual queries:} In a counterfactual query, we are interested in what the value of a variable $X_i$ would have been, had another variable $X_j$ taken a different value than it actually did. \citet{pearl_book2009} describes a three-step procedure to answer such queries. Given a structural causal model $M$ and observed evidence $e$ (i.e., the values of all variables $\mathbf{x}$ in one observed sample sample), the goal is to compute the counterfactual value or probability of $Y = y$ under the hypothetical condition $X = x$.
% 
% \vspace{1em}
% \noindent
% The three steps, known as \textbf{abduction}, \textbf{action}, and \textbf{prediction}, are:
% 
% \begin{itemize}
%     \item \textbf{Abduction:} With the model $M$ infer the latent variables $Z$ that explain the observed sample $x$.
%     \item \textbf{Action:} Modify the model $M$ by applying the intervention \( \text{do}(X_i = \alpha) \), i.e., set $X_j$ to the hypothetical value $\alpha$.
%     \item \textbf{Prediction:} Use the updated model and latent variables to compute the counterfactual value or distribution of $X_i$.
% \end{itemize}
% 
% \vspace{1em}
% \noindent
% This procedure is illustrated in Figure~\ref{fig:cf_illustration} and formalized in the pseudocode below.
% 
% % Insert your image for counterfactual here
% \begin{figure}[H]
% \centering
% \includegraphics[width=0.8\textwidth]{img/counterfactual.png}
% \caption{Illustration of the three steps in counterfactual reasoning.}
% \label{fig:cf_illustration}
% \end{figure}
% 
% \begin{algorithm}
% \caption{Answer a Single Counterfactual Query}
% \label{alg:single_cf}
% \begin{algorithmic}
% \State \textbf{Given:} A structural model \( X_k = f(Z_k, \text{pa}(X_k)) \), with inverse noise map \( Z_k = h(X_k \mid \text{pa}(X_k)) \)
% \State \textbf{Input:} Observed sample \( x \), intervention \( X_i := \alpha \), target variable \( X_j \)
% \vspace{0.5em}
% \State \textbf{Step 1: Abduction} \hspace{0.5em} Compute latent noise \( Z_j = h(x_j \mid \text{pa}(x_j)) \)
% \State \textbf{Step 2: Action} \hspace{1.6em} Replace \( x_i \) with \( \alpha \) in the parent set \( \text{pa}(X_j) \)
% \State \textbf{Step 3: Prediction} \hspace{0.3em} Compute \( x_j^{cf} = h_j^{-1}(Z_j \mid \text{pa}(X_j)^{cf}) \)
% \end{algorithmic}
% \end{algorithm}
% 
% \vspace{1em}
% \noindent
% While the \emph{distribution} of a counterfactual variable \( Y \) under a hypothetical intervention \( X = x \) can be estimated for both discrete and continuous outcomes, the \emph{actual counterfactual value} is only well-defined for continuous outcomes. For discrete variables, latent variables can be abduced, but the inverse map \( h^{-1} \) may not yield a unique category.
% 
% 
% \textbf{Counterfactual queries}: In a counterfactual query one wants to know what the value of variable $X_i$ would have been if another variable $X_j$ had a different value than what was actually observed. \citet{pearl_book2009} describes the three-step process to answer counterfacutal queries as follows: Given a causal model $M$ and observed evidence $e$ (which are the actually observed values of the variables $X_i$ of one sample) one wants to compute the probability of $Y=y$ under the hypothetical condition $X=x$.
% 
% 
% Here input image of counterfactuals from final presentation.
% 
% 
% Step 1 aims to explain the past (Z) by knowledge of the evidence e; 
% Step 2 amends the past to the hypothetical condition $X=x$ 
% Step 3 predicts the future (Y) based on our new understanding of the past and our newly established condition, $X =x$
% 
% Pearl named these three steps, (1) abduction,  (2) action and (3) prediction. The procedure is described in the pseudocode \ref{alg:counterfactual} and illustrated in Figure.
% 
% \begin{algorithm}
% \caption{Answer a Single Counterfactual Query}
% \label{alg:single_cf}
% \begin{algorithmic}[1]
% \State \textbf{Given:} A structural model $X_k = f(Z_k, \text{pa}(X_k))$, with inverse noise map $Z_k = h(X_k \mid \text{pa}(X_k))$
% \State \textbf{Input:} Observed sample $x$, intervention $X_i := \alpha$, target variable $X_j$
% \vspace{0.3em}
% \State \textbf{Step 1: Abduction} Infer latent variable $Z_j = h(x_j \mid \text{pa}(x_j))$ using the observed values
% \vspace{0.3em}
% \State \textbf{Step 2: Action} Replace the value of $X_i$ with $\alpha$ in the set of parent variables
% \vspace{0.3em}
% \State \textbf{Step 3: Prediction} Compute the counterfactual value $x_j^{cf} = h_j^{-1}(Z_j \mid \text{pa}(x_j)^{cf})$
% \vspace{0.3em}
% \end{algorithmic}
% \end{algorithm}
% 
% 
% 
% While the probability of Y under the hypothetical condition $X=x$ can be determined in any case, the actual counterfactual value of Y is only defined for a continuous outcome but not for discrete outcomes.

% see pearl book causality: 1.4.4 Counterfactuals in Functional Models (page 36)

% (What pearl writes:  Likewise, in contrast with the potential-outcome framework, counterfactuals in the structural account are not treated as undefined primitives but rather as quantities to be derived from the more fundamental concepts of causal mechanisms and their structure. )











% enforce that starts after all floats have been displayed
\FloatBarrier


\section{Individualized Treatment Effects (ITEs)} \label{sec:methods_ite}



% - Short intro to individualized treatment effects and its importance, including that RCT's only measure ATE
% - definition of Individual treatment effect with formula, including potential outcomes (Rubin causal model /potential outcomes framework)
% - CATE and ITE are the same and are estimate of the individual effect
% - note that we will use ITE for simplicity but also CATE is meant
% - formula of ITE with Probabilities for a binary treatment and outcome 
% - Introduction of quantile treatment effects, just short with formula (same expept quantile instead expected value)
% - homogeneous vs. heterogeneous treatment effects (with interaction example, already hint that despite no interaction there can still be heterogeneous treatment effects, due to covariates and nonlinear transformation)
% 
% - Assumptions: identifiability (ignorability?)
% 
% - Models for ITE estimation: meta learners (T, S, X), virtual twins
% -calibration must be good, prevent overfitting
% 
% - validation techniques for ITE estimation: ITE densities, ITE-ATE plots, simulation scatterplots, ATE on average (Confidence intervals and bootstrapping better talked about in respective experiment methods), (if ITE are correct, they should be on average the ATE, but if it is the same we cannot say certailny that the ITE are correct)
% 
% 
% 
% --> explain in detail in the discussion the phenomenon of ghost interactions, 
%  and possible solutions (IV, )




% https://ascpt.onlinelibrary.wiley.com/doi/epdf/10.1002/cpt.3159
% --> perfect introduction and overview and assumptions also in observatinoal setting about ITE with causal ML --> check it!
% also limitations and how to validate models

% Check Hooglands paper (guide) again, he should also say something about benefits of RCT that might be relevant for my subjects.

% https://arxiv.org/pdf/2108.04939
% above paper also discusses the implications of strong identifiability assumpions and that cate still can be biased if there are unobserved interaction variables. (however, not sure how good this paper is, many typos)


% RCTs are considered the gold standard for estimating the causal effect of a treatment due to their ability to eliminate confounding through randomization \citep{little2000}. However, RCTs typically aim to estimate the average treatment effect (ATE), which summarizes the effect of the treatment across the entire study population. This approach overlooks individual-level variation in treatment response: depending on their unique characteristics, some individuals may benefit substantially, others not at all, or even be harmed. 

While the homogeneous treatment effect refers to the part of the effect that is equal for all patients, the heterogeneous treatment effect describes the variation in treatment effects across individuals or subgroups. One reason for treatment effect heterogeneity is the presence of interaction variables \citep{hoogland2021}, also referred to as effect modifiers \citep{christensen2021}, which influence how the treatment effect varies depending on individual characteristics. However, even in the absence of explicit interaction terms, treatment effects can still exhibit heterogeneity in certain model classes, such as logistic regression \citep{hoogland2021}, or in other subclasses of transformation models. This phenomenon is further explored in Experiment 4 and will be discussed in detail in Section~\ref{sec:exp4_sc2}.

In personalized medicine and informed decision-making, individual-level treatment effects are also of interest besides population averages. The individual treatment effect compares the potential outcomes for an individual~$i$ under two different treatment options: the potential outcome $Y_i(1)$ under treatment, and $Y_i(0)$ under control, as defined by Rubin's potential outcomes framework~\citep{rubin2005}. The individual treatment effect is typically defined as the difference between the two potential outcomes:


\begin{equation}
Y_i(1) - Y_i(0).
\label{eq:ind_tr_ef}
\end{equation}



However, for any individual, only one of these two potential outcomes can be observed -- the other one remains counterfactual. This is known as the fundamental problem of causal inference \citep{holland1986}, which implies that the individual treatment effect (Equation~\ref{eq:ind_tr_ef}) is inherently unobservable. Estimating causal effects from observed data requires untestable assumptions to ensure identifiability, in contrast to standard predictive modeling.


\medskip 

% ...assumpitons to link observed data with unobserved counterfactuals
\textbf{Assumptions for identifiability:} A central assumption is consistency, which requires that the observed outcome equals the potential outcome under the treatment actually received: $Y = Y(1)$ if $T = 1$, and $Y = Y(0)$ if $T = 0$. The Stable Unit Treatment Value Assumption (SUTVA) assumes no interference between units and that treatments are well-defined \citep{rubin1980}. The most critical assumption is ignorability (or unconfoundedness), which states that, conditional on observed covariates $\mathbf{X}$, the treatment assignment is independent of the potential outcomes \citep{rosenbaum1983}:

\begin{equation}
(Y(1), Y(0)) \perp T \mid \mathbf{X}.
\label{eq:ignorability}
\end{equation}

Equation~\ref{eq:ignorability} implies that there are no unmeasured confounders affecting both treatment assignment and outcomes. In an RCT, this condition is typically satisfied by design. In contrast, for observational data, the covariate set $\mathbf{X}$ must contain a valid adjustment set to appropriately control for confounding. Additionally, care must be taken to avoid adjusting for collider variables, as this can introduce selection bias \citep{elwert2014, hernan2025}. Finally, the positivity assumption requires that every individual has a non-zero probability of receiving each treatment level:

\begin{equation}
0 < P(T = 1 \mid \mathbf{X} = \mathbf{x}) < 1 \quad \text{for all } \mathbf{x}.
\end{equation}

\medskip

While the individual treatment effect (Equation~\ref{eq:ind_tr_ef}) is inherently unobservable, its expectation conditional on covariates can be identified under the assumptions above. This quantity is referred to as the conditional average treatment effect (CATE), or individualized treatment effect (ITE). In this thesis, we use the term ITE to denote this estimand. For an individual with covariates \( \mathbf{X} = \mathbf{x}_i \), the ITE can be identified as:

\medskip



\begin{align}
\label{eq:ite}
\text{ITE}(\mathbf{x}_i) &= \mathbb{E}[Y_i(1) - Y_i(0) \mid \mathbf{X} = \mathbf{x}_i] \notag \\[0.5em]
&= \mathbb{E}[Y_i(1) \mid \mathbf{X} = \mathbf{x}_i] - \mathbb{E}[Y_i(0) \mid \mathbf{X} = \mathbf{x}_i] \notag \\[0.5em]
&= \mathbb{E}[Y_i(1) \mid T = 1, \mathbf{X} = \mathbf{x}_i] 
 - \mathbb{E}[Y_i(0) \mid T = 0, \mathbf{X} = \mathbf{x}_i] \quad \text{(by ignorability)} \notag \\[0.5em]
&= \mathbb{E}[Y_i \mid T = 1, \mathbf{X} = \mathbf{x}_i] 
 - \mathbb{E}[Y_i \mid T = 0, \mathbf{X} = \mathbf{x}_i] \quad \text{(by consistency)}
\end{align}



% 
% \begin{equation}
% \text{ITE}_i(\mathbf{x}_i) = \mathbb{E}[Y_i(1) - Y_i(0) \mid \mathbf{X}_i = \mathbf{x}_i] 
% = \mathbb{E}[Y_i(1)\mid \mathbf{X}_i = \mathbf{x}_i] - \mathbb{E}[Y_i(0)\mid \mathbf{X}_i = \mathbf{x}_i]
% \end{equation}
% 
% It reflects the expected, or average, treatment effect for an individual~$i$, conditional on that individual's specific covariates~$\mathbf{X}_i = \mathbf{x}_i$.
% 
% 
% \noindent
% Under the assumption of \textbf{ignorability} (also called unconfoundedness), that is, $(Y_i(1), Y_i(0)) \perp T_i \mid \mathbf{X}_i$, the expression can be rewritten as:
% 
% \begin{equation}
% \text{ITE}_i(\mathbf{x}_i) = \mathbb{E}[Y_i(1) \mid T_i = 1, \mathbf{X}_i = \mathbf{x}_i] 
% - \mathbb{E}[Y_i(0) \mid T_i = 0, \mathbf{X}_i = \mathbf{x}_i]
% \end{equation}
% 
% \medskip
% 
% \noindent
% Finally, under the \textbf{consistency} assumption, we can substitute $Y_i = Y_i(1)$ and $ Y_i = Y_i(0)$:
% 
% \begin{equation}
% \text{ITE}_i(\mathbf{x}_i) = \mathbb{E}[Y_i \mid T_i = 1, \mathbf{X}_i = \mathbf{x}_i] 
% - \mathbb{E}[Y_i \mid T_i = 0, \mathbf{X}_i = \mathbf{x}_i]
% \end{equation}

\medskip

In the case of a binary outcome $Y$, where the expected value corresponds to the probability that $Y = 1$, the ITE for an individual with covariates $\mathbf{X} = \mathbf{x}_i$, under the identifiability assumptions, is given by \citep{hoogland2021}:

\medskip

\begin{equation}
\text{ITE}(\mathbf{x}_i) = P(Y_i = 1 \mid T = 1, \mathbf{X} = \mathbf{x}_i) - P(Y_i = 1 \mid T = 0, \mathbf{X} = \mathbf{x}_i).
\label{eq:ITE_binary}
\end{equation}

\medskip

If $Y = 1$ indicates recovery, a positive ITE means that the individual is expected to benefit from the treatment compared to control. A negative ITE suggests a lower probability of recovery under treatment, while an ITE of zero indicates no expected difference between treatment and control for that individual.



\medskip




% 
% RCTs are considered the gold standard for estimating the causal effect of a treatment due to their ability to eliminate confounding through randomization \citep{little2000}. However, RCTs typically aim to estimate the average treatment effect (ATE), which summarizes the effect of the treatment across the entire study population. This approach overlooks individual-level variation in treatment response: depending on their unique characteristics, some individuals may benefit substantially, others not at all, or even be harmed. While the homogeneous treatment effect refers to the part of the effect that is equal for all patients, the heterogeneous treatment effect (HTE) describes the variation in treatment effects across individuals or subgroups. One reason for treatment effect heterogeneity is the presence of interaction variables \citep{hoogland2021}, also referred to as effect modifiers \citep{christensen2021}, which influence how the treatment effect varies depending on individual characteristics. However, even in the absence of explicit interaction variables, treatment effects can still exhibit heterogeneity due to the scale on which personalized effects are evaluated \citep{hoogland2021}. This phenomenon is further explored in Experiment 4 and will be discussed in detail in Section~\ref{sec:disc_experiment4}.
% 
% In personalized medicine and informed decision-making, individual-level treatment effects are more relevant than population averages. The individual treatment effect compares the potential outcomes for an individual~$i$ under two different treatment options: the potential outcome $Y_i(1)$ under treatment, and $Y_i(0)$ under control, as defined by Rubin's potential outcomes framework~\citep{rubin2005}. It is typically defined as the difference between the two potential outcomes:
% 
% 
% \begin{equation}
% Y_i(1) - Y_i(0).
% \end{equation}
% 
% 
% 
% However, only one of the two potential outcomes can be observed for each individual, which is known as the fundamental problem of causal inference~\citep{holland1986}. Instead, most methods aim to estimate the expected treatment effect conditional on a given set of covariates~$\mathbf{X}$. This quantity is known as the conditional average treatment effect (CATE), which is valid under assumptions discussed later:
% 
% 
% \begin{align}
% \text{CATE}_i(\mathbf{x}_i) &= \mathbb{E}[Y_i(1) - Y_i(0) \mid \mathbf{X}_i = \mathbf{x}_i] \notag \\
%                  &= \mathbb{E}[Y_i \mid T = 1, \mathbf{X}_i = \mathbf{x}_i] - \mathbb{E}[Y_i \mid T = 0, \mathbf{X}_i = \mathbf{x}_i]
% \end{align}
% 
% 
% It reflects the expected, or average, treatment effect for an individual~$i$, conditional on that individual's specific covariates~$\mathbf{X}_i = \mathbf{x}_i$.
% 
% In fields such as healthcare, the CATE is often referred to as the individualized treatment effect (ITE), as for example done by~\citet{hoogland2021}. In this thesis, we use the term ITE to denote individualized treatment effects, i.e., CATE.
% 
% The ITE for an individual with covariates $\mathbf{X}_i = \mathbf{x}_i$ in the case of a binary outcome and binary treatment is defined as:
% 
% \begin{equation}
% \text{ITE}_i(\mathbf{x}_i) = P(Y_i = 1 \mid T = 1, \mathbf{X}_i = \mathbf{x}_i) - P(Y_i = 1 \mid T = 0, \mathbf{X}_i = \mathbf{x}_i).
% \end{equation}
% 
% Assuming $Y = 1$ indicates recovery, a positive ITE means the individual is expected to benefit from the treatment compared to control. A negative ITE suggests the outcome is expected to be worse under treatment, while an ITE of zero implies no expected difference between treatment and control for that individual.
% 
% \medskip
% 
% \textbf{Assumptions for Identifiability:} Unlike standard predictive modeling, the estimation of the individualized treatment effect relies on untestable assumptions to ensure identifiability of causal effects from observational data. A central assumption is consistency, which requires that the observed outcome equals the potential outcome under the received treatment: $Y = Y(1)$ if $T = 1$, and $Y = Y(0)$ if $T = 0$. The Stable Unit Treatment Value Assumption (SUTVA) assumes no interference between units and that treatments are well-defined \citep{rubin1980}. The most critical assumption is ignorability (or unconfoundedness), which states that, conditional on observed covariates $X$, the treatment assignment is independent of the potential outcomes \citep{rosenbaum1983}:
% 
% \begin{equation}
% (Y(1), Y(0)) \perp T \mid X.
% \end{equation}
% 
% This implies that there are no unmeasured confounders affecting both treatment and outcome. Additionally, the positivity assumption requires that every individual has a non-zero probability of receiving each treatment level:
% 
% \begin{equation}
% 0 < P(T = 1 \mid X = x) < 1 \quad \text{for all } x.
% \end{equation}
% 
% 
% \medskip
% 

% An example of ITE estimation using a TRAM-DAG is provided in Appendix~\ref{appendix:ite_tram_example}.

\textbf{ITE estimation:} In practice, the ITE (Equation~\ref{eq:ite}) is estimated by fitting a model to the observed data and then predicting the outcome under both treatment conditions. For binary outcomes, this requires a model that predicts the probability of $Y = 1$ under treatment and control (i.e., the expected value of a binary outcome; see Equation~\ref{eq:ITE_binary}). For continuous outcomes, a model is required that predicts the expected value of each potential outcome. Alternatively, one may compute the difference in the predicted medians of the potential outcomes, corresponding to the quantile treatment effect (QTE) \citep{chernozhukov2005} at the median (Equation~\ref{eq:qte}). This is illustrated in the simulation example in Experiment~4 (see Section~\ref{ch:exp4}). An overview of the models used for ITE estimation is provided below.


\medskip

\textbf{Models for ITE estimation:} \label{sec:ite_models} While many other approaches for ITE estimation exist, this thesis focuses on two widely used types of metalearners: the T-learner and the S-learner \citep{kunzel2019}. These methods are model-agnostic, meaning that any predictive algorithm can be used as a base model. The T-learner fits \textbf{t}wo separate models: one on the treated group and one on the control group. If the treatment was assigned randomly, confounding should not be a problem. The ITE for an individual is then estimated as the difference between the predicted outcomes from these two models (as in Equation~\ref{eq:ite}). In contrast, the S-learner fits a \textbf{s}ingle model on all individuals, incorporating the treatment indicator as an additional covariate to distinguish between treatment conditions. Therefore, the S-learner requires a model that is complex enough to capture interaction effects between treatment and covariates. Once the S-learner is fitted, both potential outcomes for each individual can be estimated by making two predictions: one under the intervention $\text{do}(T = 0)$ and one under $\text{do}(T = 1)$.

The experiments in this thesis were conducted using both S- and T-learners. For binary outcomes, we used models based on logistic regression, logistic lasso regression, random forests, and TRAM-DAGs to predict the probabilities of $Y = 1$ for the potential outcomes (see Experiments~2 and~3; Sections~\ref{ch:exp2} and~\ref{ch:experiment3}). For continuous outcomes, we used TRAM-DAGs, which—when applied as S-learners—can flexibly model treatment effect heterogeneity when specified with complex shift or complex intercept terms. For simplicity, the ITEs for continuous outcomes were computed as the difference in the predicted medians of the potential outcomes (Equation~\ref{eq:qte}), although estimation based on expected values would also have been possible, as discussed in the ITE estimation procedure in Experiment~4 (see Section~\ref{sec:methods_experiment4}).



Accurate ITE estimation requires models that generalize well, which in turn means avoiding overfitting. Since ITEs are computed as differences between two outcome predictions, even small errors can accumulate. \citet{hoogland2021} emphasize the need to balance model complexity with data availability to avoid overfitting. They also highlight that estimating risk differences is inherently more challenging than predicting single outcomes. Furthermore, calibration is crucial when ITEs are derived from predicted potential outcomes \citep{hoogland2024}. While neural networks may be accurate, they often produce poorly calibrated probabilities \citep{guo2017}, which can mislead treatment effect estimation. Similarly, conventional models like logistic regression can overfit in small samples or high-dimensional settings, leading to extreme predictions. Penalization methods such as lasso shrink coefficients to improve generalization \citep{riley2021}, though studies have shown that they can perform inconsistently in low-sample scenarios \citep{calster2020}. These factors are important to consider when estimating ITEs.


When using random forests, proper tuning of hyperparameters like tree depth and variable selection (e.g., \texttt{mtry}) may be necessary to avoid overfitting.
In summary, model choice for ITE estimation should consider the complexity of treatment-outcome relationships, sample size, and calibration needs. While complex models like TRAM-DAGs and random forests can capture complicated heterogeneity, simpler models may offer more robust performance in smaller datasets or where treatment-covariate interactions are straightforward.



% 
% 
% A variety of methods are applied to estimate ITEs; in this thesis we focus on metalearners \citep{kunzel2019}, in particular S-learners and T-learner. S-learners use a single model that uses the treatment as covariate to predict both potential outcomes. T-learners fit two separate models, one on each treatment group. These two methods are model agnostic, hence any type of model can be used. In this thesis follwoing models were applied: TRAM-DAGs, logistic regression, lasso regression, and random forests.
% 
% \citet{hoogland2021} highlighted the importance of considering the model complexity and the susceptibility to overfitting. For ITE estimation, the models must generalize to a new sample. 
% The ITE for a binary endpoint is estimated as the difference of two probabilities (the risk under treatment minus the risk under control). It is essential that the model used to estimate these probabilities is well calibrated and generalizes to new (unseen) data. \citep{guo2017} point out that even though modern neural networks became much more accurate in terms of prediction performance, they are no longer well-calibrated. As it may not be a big problem for the sole purpose of making good predictions, it is very problematic for applications where an accurate quantification of the uncertainty is needed. When using models that are estimated with conventional methods such as ordinary least squares or standard maximum likelihood, they tend to overfit on the training data and make too extreme predictions on the test data. This problem increases with reduced sample size, low event rate or large number of predictor variables. To prevent such overfitting in regression models, penalization (shrinkage) methods are proposed as they shrink the estimated coefficients towards zero to reduce the variance in predictions on new data \citep{riley2021}. 
% 
% 
% \citet{curth2024} provide a comprehensive overview of the individualized treatment effect (ITE) and its estimation in the context of causal machine learning. They state its importance in comparison to average treatment effects, the assumpitons that need to be fulfilled, what kind of limiations typically are encountered and how models should be validated.

% take this paper as guide to the structure of this section to explain overview and concept
% https://ascpt.onlinelibrary.wiley.com/doi/epdf/10.1002/cpt.3159

% another paper of Alicia Curth https://proceedings.mlr.press/v130/curth21a/curth21a.pdf



% https://pmc.ncbi.nlm.nih.gov/articles/PMC5074325/ explained how to calibrate random forests for probability estimation (or just explain what comets does, maybe enough)


% Logistic regression, penalized logistic regression (shrinkage, lasso
% Shrinkage methods should provide better predictive perfomance on average (cite articles). \citet{calster2020} analyzed different regression shrinkage methods with a binary outcome in a simulation study. They concluded, although the calibration slope improved on average, shrinkage often worked poorly on individual datasets. With small sample size and low number of events per variable the performance of most of these methods were highly variable and should be used with caution in such settings. \citet{riley2021} obtained to similar results in their simulation study. Problems occur, because tuning parameters are often estimated with large uncertainty on the training data and fail to generalize. In both studies the autors pointed out that these penalization methods are more unreliable when needed most, that is when the risk of overfitting may be large.
% 
% When using a Random Forest based method as an S-learner (where Treatment is given as input variable) one has to make sure that the treatment variable is really used in the forests and not left out by mtry (not all variables are used for splits because decorrelating and reducing overfit.)
% 
% (Shrinkage shrinks the coefficients so that the calibration slope is improved on a test set. The shrinkage factor can for example be found with n-fold cross validation, as e.g. done by lasso with L1 penalization)
% 
% Explain tuning of random forest, with depth number of trees and mtry
% 
% TRAM-DAGs with complex shifts or compmlex intercepts can capture heterogeneity. See appendix XXX for an example of ITE estimation with a complex shift.



% Explain the kinds of validation measures & plots so that i dont have to explain them again in the next sections (experiments)


\medskip

\textbf{Validation of ITE estimation:} \label{sec:ite_validation} \citet{hoogland2024} examined a variety of evaluation methods for assessing discrimination and calibration in ITE estimation. They emphasize that accurate prediction of potential outcomes should be considered a prerequisite for reliable ITE modeling. Furthermore, as in standard prediction modeling, proper validation should ideally be performed on independent (out-of-sample) data.
In this thesis, although various quantitative evaluation metrics exist, we primarily relied on visual validation tools. 
One such approach is the ITE-ATE plot (see, e.g., Figure~\ref{fig:IST_density_ITE_ATE_glm_tlearner}), where individuals are grouped based on their predicted ITEs. Within each ITE-subgroup, the empirical ATE is calculated using observed outcomes. Depending on the context, the empirical ATE is calculated on the same scale as the predicted ITEs -- for example, as a risk difference for binary outcomes or an absolute difference for continuous outcomes. If the ITE estimates are well-calibrated, the observed ATEs should align with the predicted values, resulting in a calibration curve that lies along the identity line. When estimating ITEs on real-world data, where the true ITEs are unknown, ITE-ATE plots serve to validate whether the estimated ITEs actually generalize with respect to observed outcomes when evaluated on an independent test set. However, ITE-ATE plots cannot reveal if there is additional treatment effect heterogeneity present that was not captured by the model. This limitation will be discussed on a simulated example in Experiment 3 (Scenario 3.2, Section~\ref{sec:exp3_sc2}).
In simulation experiments with known true ITEs, estimation accuracy was assessed by directly comparing predicted and true ITEs -- providing a complete picture beyond the limitations of ITE-ATE plots, though only feasible in simulated settings. 
Furthermore, model calibration in terms of predictive performance $P(Y \mid \mathbf{X} = \mathbf{x})$ was evaluated using calibration plots. As an additional check, the average of the estimated ITEs across the population should equal the estimated ATE, i.e., $\mathbb{E}[\text{ITE}(X)] = \text{ATE}$. Large deviations from this condition may indicate systematic bias. However, satisfying this equality does not guarantee that the estimated ITEs are accurate at the individual level. All evaluations were conducted both on the training data and on a separate hold-out test set.

% It should be noted, that when estimating the ITEs , the mean of the ITEs across the population should equal the ATE, i.e., $\mathbb{E}[\tau(X)] = \text{ATE}$. However, if one estimated ITEs whose average is equal to the ATE in a population, this does not neccesarily mean that the estimated ITEs are correct. 
% 
% See \citep{hoogland2024} for a analyzis of ITE estimation validation methods. He mentions it is best to check out of sample. 
% 
% In this thesis we rely mostly on visual validation tools such as ITE-ATE plots where individuals are grouped according to the predicted ITE and in each ITE subgroup the ATE is computed according to the actually observed outcomes. If ITE estimation is correct, the ATE in each subgroup should be equal to the predicted ITE, hence exhibit a calibration line on the identity line. Furthermore, in the experiments based on simulations, the true ITE is known, which allows to compare the estimated ITEs with the true ITEs. 
% 



% Assessing predictive performance with AUC, calibration slope, and brier score. In Leo presentation, he says that recalibration can either be done with deviance statistics or by leave one out (loo) cross validation the slope of this regression would then be the estimated shrinkage factor.




% If ITEs are correct, they should give on average the ATE: E(ITE) = ATE, (cite a paper, maybe Alicia Curth or Hoogland?). However, we can not say that the ITEs must be correct, if they give the ATE on average. (because they could for example also be much more spread than the true ITEs but still give the correct ATE on average!)







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% - also talk about propensity score Rubin(2007) to basically estimate an RCT...and overcome the problem of confounding. but this might work for ATE but not really for ITE, direct modelling of the outcome is necessary % https://pmc.ncbi.nlm.nih.gov/articles/PMC5920646/


% https://journals.sagepub.com/doi/pdf/10.1177/1536867X1001000309 also explain how to do quantile treatment effect QTE estimation with instrumental variables etc



% In this thesis , I will apply Lasso regression on the IST stroke trial and simulation studies, where the sample size is relatively large.


% simulation studies
%  "The setup was such that development and test sets were generated from the same data generating mechanism. In practice, there may be differences between these two settings that are not captured by the models, and the uncertainty that accompanies these unknowns may overshadow relatively small gains realized by more complex models."
%  % https://pmc.ncbi.nlm.nih.gov/articles/PMC9291969/#sim9154-bib-0065
 
 % "This could include the analysis of individual patient data from multiple randomized trials, or even the use of nonrandomized studies for the estimation of outcome risk under a control condition." this motivates the need for observational modeling.


% Problems with ITE: (in an RCT setting)
% - to estimate the ITE we must assume un-confoundedness. Does this also apply to itneractions (effect modifiers)? Check how this is handled in the literature.
% https://journals.lww.com/epidem/fulltext/2007/09000/four_types_of_effect_modification__a.6.aspx talks about different types of effect modifiers (not sure if this really contributes to the thesis)






% 
% 

% 
% 


% \subsection{Propensity Score Adjustment in Observational Settings}
% 
% In the absence of randomization, the \textit{propensity score}, defined as the probability of receiving treatment conditional on covariates $X$,
% \begin{equation}
% e(X) = P(T = 1 \mid X),
% \end{equation}
% can be used to balance treatment groups and reduce confounding rosenbaum1983central. When the ignorability assumption holds, adjusting for the propensity score allows for unbiased estimation of average treatment effects. However, while propensity score methods (e.g., matching, stratification, weighting) are effective for estimating population-level quantities like the ATE, they are often insufficient for ITE estimation. Since ITE requires modeling both potential outcomes at the individual level, direct modeling approaches such as outcome regression, meta-learners, or Bayesian models are typically more appropriate rubin2007design, nie2021quasi. Moreover, reliance on the propensity score alone may fail to capture fine-grained individual heterogeneity necessary for personalized treatment decisions ali2019addressing.









\section{Software}


All code used in this thesis is available on GitHub: \url{https://github.com/mikekr97/MA_Mike}.

All analyses were conducted in R~\citep{R} (version 4.4.2) using RStudio. The packages \texttt{keras}~\citep{keras} (version 2.15.0), \texttt{tensorflow}~\citep{tensorflow} (version 2.16.0), and \texttt{reticulate}~\citep{reticulate} (version 1.40.0) were used to build and train neural networks through Python's TensorFlow backend. These tools allowed for the use of deep learning methods directly within the R environment.


% Maybe it is the methods section. Here however, we give a couple hints.
% Note that you can wisely use \rr{preamble}-chunks. Minimal, is likely:
% 
% 
% \bigskip
% 
% \hrule
% <<echo=TRUE>>=
% library(knitr)
% opts_chunk$set(
%     fig.path='figure/ch02_fig',
%     self.contained=FALSE,
%     cache=TRUE
% )
% @
% \hrule
% 
% \bigskip
% 
% Defining figure options is very helpful:
% 
% 
% \bigskip
% 
% 
% \hrule
% <<echo=TRUE,cache=FALSE>>=
% library(knitr)
% opts_chunk$set(fig.path='figure/ch02_fig',
%                echo=TRUE, message=FALSE,
%                fig.width=8, fig.height=2.5,
%                out.width='\\textwidth-3cm',
%                message=FALSE, fig.align='center',
%                background="gray98", tidy=FALSE, #tidy.opts=list(width.cutoff=60),
%                cache=TRUE
% )
% options(width=74)
% @
% \hrule
% 
% \bigskip
% 
% This options are best placed in the main document at the beginning. Otherwise a \verb+cache=FALSE+ as knitr option is necessary to overrule a possible  \verb+cache=TRUE+ flag.
% 
% \bigskip
% 
% Notice how in Figure~\ref{f02:1} everything is properly scaled.
% 
% \begin{figure}
% <<echo=FALSE>>=
% par(mai=c(.8,.8,.1,.1))
% set.seed(12)
% plot( runif(30), type='l')
% @
%   \caption{Test figure to illustrate figure options used by knitr.}
%   \label{f02:1}
% \end{figure}
% 
% 
% \section{Citations}
% 
% Recall the difference between \verb+\citet{}+ (e.g., \citet{Chu:Geor:99}), \verb+\citep{}+ (e.g., \citep{Chu:Geor:99}) and \verb+\citealp{}+ (e.g., \citealp{Chu:Geor:99}).
% For simplicity, we include here all references in the file \verb+biblio.bib+ with the command \verb+\nocite{*}+.\nocite{*}

