% Some sample entries...

% Be consistent with either Journal abbreviations or full descriptions.
% `Number` may be missing for some journal articles.
% Carful with captialization of journal titles, use, e.g., {Bayesian}, {Markov} chain {Monte Carlo}, ...

% There is no unique rule whether to include the address of the publisher for books. Be consistent.\
% example Chu:Geor:99 is the most minimal one.

% Additional fields may be present. Spacing and capitalization of the fields are not relevant. 

@ARTICLE{Porc:Furr:Nych:21,
  AUTHOR = 	 {Emilio Porcu and Reinhard Furrer and Douglas Nychka},
  TITLE = 	 {30 Years of Space-time Covariance Functions},
  JOURNAL = 	 {Wiley Interdiscip. Rev. Comput. Stat.},
  FJOURNAL = 	 {Wiley Interdisciplinary Reviews. Computational Statistics},
  YEAR = 	 {2020},
  DOI =          {10.1002/wics.1512},
  VOLUME = 	 {13:e1512},
  PAGES = 	 {1--24},
}


@INPROCEEDINGS{Wang:Furr:20,
  AUTHOR = 	 {Craig Wang and Reinhard Furrer},
  TITLE = 	 {Monte {Carlo} Permutation Tests for Assessing Spatial Dependence at Different Scales},
  EDITOR =       {{La Rocca}, M. and Liseo, B. and Salmaso, L.},
  BOOKTITLE =    {Nonparametric Statistics. ISNPS 2018. Springer Proceedings in Mathematics \& Statistics},
  PUBLISHER =    {Springer},
  YEAR = 	 {2020},
  DOI =          {10.1007/978-3-030-57306-5_45},
  VOLUME = 	 {339},
  PAGES = 	 {503--511},
}

@Manual{varrank,
    title = {varrank: an {R} package for variable ranking based on mutual
             information with applications to observed systemic datasets},
    author = {Gilles Kratzer and Reinhard Furrer},
    year = {2018},
    note = {R package version 0.3},
    url = {https://CRAN.R-project.org/package=varrank},
}


@inbook {Hurr:etal:13,
title = {An Overview of the {North} {Atlantic} Oscillation},
author = {Hurrell, James W. and Kushnir, Yochanan and Ottersen, Geir and Visbeck, Martin},
publisher = {American Geophysical Union},
isbn = {9781118669037},
doi = {10.1029/134GM01},
pages = {1--35},
booktitle = {The North Atlantic Oscillation: Climatic Significance and Environmental Impact},
year = {2013},
}


@Book{Chu:Geor:99,
  author = 	 {Eleanor Chu and Alan George},
  title = 	 {Inside the FFT Black Box: Serial and Parallel Fast Fourier Transform Algorithms},
  publisher = 	 {CRC Press},
  year = 	 {1999},
}

@book{Coll:14,
  title={Modelling Survival Data in Medical Research},
  edition={Third},
  author={Collett, D.},
  series={Chapman \& Hall/CRC Texts in Statistical Science},
  year={2014},
  publisher={Taylor \& Francis}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{pearl2009,
author = {Judea Pearl},
title = {{Causal inference in statistics: An overview}},
volume = {3},
journal = {Statistics Surveys},
number = {none},
publisher = {Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
pages = {96 -- 146},
keywords = {causal effects, causes of effects, confounding, counterfactuals, graphical methods, mediation, policy evaluation, potential-outcome, structural equation models},
year = {2009},
doi = {10.1214/09-SS057},
URL = {https://doi.org/10.1214/09-SS057}
}

@article{dawid2000,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2669377},
 abstract = {A popular approach to the framing and answering of causal questions relies on the idea of counterfactuals: outcomes that would have been observed had the world developed differently; for example, if the patient had received a different treatment. By definition, one can never observe such quantities, nor assess empirically the validity of any modeling assumptions made about them, even though one's conclusions may be sensitive to these assumptions. Here I argue that for making inference about the likely effects of applied causes, counterfactual arguments are unnecessary and potentially misleading. An alternative approach, based on Bayesian decision analysis, is presented. Properties of counterfactuals are relevant to inference about the likely causes of observed effects, but close attention then must be given to the nature and context of the query, as well as to what conclusions can and cannot be supported empirically. In particular, even in the absence of statistical uncertainty, such inferences may be subject to an irreducible degree of ambiguity.},
 author = {A. P. Dawid},
 journal = {Journal of the American Statistical Association},
 number = {450},
 pages = {407--424},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Causal Inference Without Counterfactuals},
 urldate = {2025-07-03},
 volume = {95},
 year = {2000}
}


@article{hariton2018,
author = {Hariton, Eduardo and Locascio, Joseph J},
title = {Randomised controlled trials - the gold standard for effectiveness research},
journal = {BJOG: An International Journal of Obstetrics \& Gynaecology},
volume = {125},
number = {13},
pages = {1716 -- 1716},
doi = {https://doi.org/10.1111/1471-0528.15199},
url = {https://obgyn.onlinelibrary.wiley.com/doi/abs/10.1111/1471-0528.15199},
eprint = {https://obgyn.onlinelibrary.wiley.com/doi/pdf/10.1111/1471-0528.15199},
year = {2018}
}


@article{nichols2007,
author = {Austin Nichols},
title ={Causal Inference with Observational Data},
journal = {The Stata Journal},
volume = {7},
number = {4},
pages = {507 -- 541},
year = {2007},
doi = {10.1177/1536867X0800700403},
URL = { https://doi.org/10.1177/1536867X0800700403},
eprint = { https://doi.org/10.1177/1536867X0800700403},
abstract = { Problems with inferring causal relationships from nonexperimental data are
                    briefly reviewed, and four broad classes of methods designed to allow estimation
                    of and inference about causal parameters are described: panel regression,
                    matching or reweighting, instrumental variables, and regression discontinuity.
                    Practical examples are offered, and discussion focuses on checking required
                    assumptions to the extent possible. }
}



@article {freedman1987,
	Title = {Equipoise and the ethics of clinical research},
	Author = {Freedman, Benjamin},
	DOI = {10.1056/nejm198707163170304},
	Number = {3},
	Volume = {317},
	Month = {July},
	Year = {1987},
	Journal = {The New England journal of medicine},
	ISSN = {0028-4793},
	Pages = {141--145},
	Abstract = {The ethics of clinical research requires equipoise--a state of genuine uncertainty on the part of the clinical investigator regarding the comparative therapeutic merits of each arm in a trial. Should the investigator discover that one treatment is of superior therapeutic merit, he or she is ethically obliged to offer that treatment. The current understanding of this requirement, which entails that the investigator have no "treatment preference" throughout the course of the trial, presents nearly insuperable obstacles to the ethical commencement or completion of a controlled trial and may also contribute to the termination of trials because of the failure to enroll enough patients. I suggest an alternative concept of equipoise, which would be based on present or imminent controversy in the clinical community over the preferred treatment. According to this concept of "clinical equipoise," the requirement is satisfied if there is genuine uncertainty within the expert medical community--not necessarily on the part of the individual investigator--about the preferred treatment.},
	URL = {https://doi.org/10.1056/NEJM198707163170304}
}




@InProceedings{gutierrez2017,
  title = 	 {Causal Inference and Uplift Modelling: A Review of the Literature},
  author = {Pierre Gutierrez and Jean-Yves G{\'e}rardy},
  booktitle = 	 {Proceedings of The 3rd International Conference on Predictive Applications and APIs},
  pages = 	 {1--13},
  year = 	 {2017},
  volume = 	 {67},
  month = 	 {11--12 Oct},
  publisher =    {Proceedings of Machine Learning Research},
  pdf = 	 {http://proceedings.mlr.press/v67/gutierrez17a/gutierrez17a.pdf},
  url = 	 {https://proceedings.mlr.press/v67/gutierrez17a.html},
  abstract = 	 {Uplift modeling refers to the set of techniques used to model the incremental impact of an action or treatment on a customer outcome. Uplift modeling is therefore both a Causal Inference problem and a Machine Learning one. The literature on uplift is split into 3 main approaches - the Two-Model approach, the Class Transformation approach and modeling uplift directly. Unfortunately, in the absence of a common framework of causal inference and notation, it can be quite difficult to assess those three methods. In this paper, we use the Rubin (1974) model of causal inference and its modern “econometrics” notation to provide a clear comparison of the three approaches and generalize one of them. To our knowledge, this is the first paper that provides a unified review of the uplift literature. Moreover, our paper contributes to the literature by showing that, in the limit, minimizing the Mean Square Error (MSE) formula with respect to a causal effect estimator is equivalent to minimizing the MSE in which the unobserved treatment effect is replaced by a modified target variable. Finally, we hope that our paper will be of use to researchers interested in applying Machine Learning techniques to causal inference problems in a business context as well as in other fields: medicine, sociology or economics.}
}

@misc{zhao2020,
      title={Uplift Modeling for Multiple Treatments with Cost Optimization}, 
      author={Zhenyu Zhao and Totte Harinen},
      year={2020},
      eprint={1908.05372},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
  note         = {arXiv preprint 1908.05372},
      url={https://arxiv.org/abs/1908.05372}, 
}

@misc{sick2025,
  author       = {Beate Sick and Oliver D{\"u}rr},
  title        = {Interpretable Neural Causal Models with {TRAM-DAGs}},
  note         = {arXiv preprint 2503.16206, accepted at the CLeaR 2025 Conference},
  year         = {2025},
  eprint       = {2503.16206},
  archivePrefix= {arXiv},
  primaryClass = {stat.ML},
  url          = {https://arxiv.org/abs/2503.16206},
  doi          = {10.48550/arXiv.2503.16206}
}


@book{pearl_book2009,
author = {Pearl, Judea},
title = {Causality: Models, Reasoning and Inference},
year = {2009},
isbn = {052189560X},
publisher = {Cambridge University Press},
address = {USA},
edition = {2nd},
abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 3,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.}
}


@inproceedings{poinsot2024,
  author    = {Audrey Poinsot and Alessandro Leite and Nicolas Chesneau and Mich{\`e}le S{\'e}bag and Marc Schoenauer},
title = {Learning structural causal models through deep generative models: Methods, guarantees, and challenges},
year = {2024},
isbn = {978-1-956792-04-1},
url = {https://doi.org/10.24963/ijcai.2024/907},
doi = {10.24963/ijcai.2024/907},
abstract = {This paper provides a comprehensive review of deep structural causal models (DSCMs), particularly focusing on their ability to answer counterfactual queries using observational data within known causal structures. It delves into the characteristics of DSCMs by analyzing the hypotheses, guarantees, and applications inherent to the underlying deep learning components and structural causal models, fostering a finer understanding of their capabilities and limitations in addressing different counterfactual queries. Furthermore, it highlights the challenges and open questions in the field of deep structural causal modeling. It sets the stages for researchers to identify future work directions and for practitioners to get an overview in order to find out the most appropriate methods for their needs.},
booktitle = {Proceedings of the 33rd International Joint Conference on Artificial Intelligence (IJCAI)},
articleno = {907},
numpages = {9},
location = {Jeju, Korea}
}




@article{hothorn2014,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/24772743},
 abstract = {The ultimate goal of regression analysis is to obtain information about the conditional distribution of a response given a set of explanatory variables. This goal is, however, seldom achieved because most established regression models estimate only the conditional mean as a function of the explanatory variables and assume that higher moments are not affected by the regressors. The underlying reason for such a restriction is the assumption of additivity of signal and noise. We propose to relax this common assumption in the framework of transformation models. The novel class of semiparametric regression models proposed herein allows transformation functions to depend on explanatory variables. These transformation functions are estimated by regularized optimization of scoring rules for probabilistic forecasts, e.g. the continuous ranked probability score. The corresponding estimated conditional distribution functions are consistent. Conditional transformation models are potentially useful for describing possible heteroscedasticity, comparing spatially varying distributions, identifying extreme events, deriving prediction intervals and selecting variables beyond mean regression effects. An empirical investigation based on a heteroscedastic varying-coefficient simulation model demonstrates that semiparametric estimation of conditional distribution functions can be more beneficial than kernel-based non-parametric approaches or parametric generalized additive models for location, scale and shape.},
 author = {Torsten Hothorn and Thomas Kneib and Peter B{\"u}hlmann},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {1},
 pages = {3--27},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Conditional transformation models},
 urldate = {2025-05-02},
 volume = {76},
 year = {2014}
}

@INPROCEEDINGS{sick2020,
  author={Sick, Beate and Hothorn, Torsten and D{\"u}rr, Oliver},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Deep transformation models: Tackling complex regression problems with neural network based transformation models}, 
  year={2021},
  volume={},
  number={},
  pages={2476-2481},
  keywords={Deep learning;Maximum likelihood estimation;Uncertainty;Neural networks;Medical services;Predictive models;Probabilistic logic},
  doi={10.1109/ICPR48806.2021.9413177}}

@misc{chen2025,
      title={Causal Machine Learning Methods for Estimating Personalised Treatment Effects -- Insights on validity from two large trials}, 
      author={Hongruyu Chen and Helena Aebersold and Milo Alan Puhan and Miquel Serra-Burriel},
      year={2025},
      eprint={2501.04061},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
  note = {arXiv preprint 2501.04061},
      url={https://arxiv.org/abs/2501.04061}, 
}



@article{little2000,
  author    = {Roderick J. Little and Donald B. Rubin},
  title     = {Causal Effects in Clinical and Epidemiological Studies Via Potential Outcomes: Concepts and Analytical Approaches},
  journal   = {Annual Review of Public Health},
  year      = {2000},
  volume    = {21},
  pages     = {121--145},
  doi       = {10.1146/annurev.publhealth.21.1.121},
  url       = {https://www.annualreviews.org/content/journals/10.1146/annurev.publhealth.21.1.121},
  publisher = {Annual Reviews},
  issn      = {1545-2093},
  keywords  = {randomization, observational studies, statistical inference, noncompliance, missing data}
}

@article{hoogland2021,
author = {Hoogland, Jeroen and IntHout, Joanna and Belias, Michail and Rovers, Maroeska M. and Riley, Richard D. and E. Harrell Jr, Frank and Moons, Karel G. M. and Debray, Thomas P. A. and Reitsma, Johannes B.},
title = {A tutorial on individualized treatment effect prediction from randomized trials with a binary endpoint},
journal = {Statistics in Medicine},
volume = {40},
number = {26},
pages = {5961-5981},
keywords = {causal inference, personalized medicine, prediction, regression, treatment effect},
doi = {https://doi.org/10.1002/sim.9154},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9154},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.9154},
abstract = {Randomized trials typically estimate average relative treatment effects, but decisions on the benefit of a treatment are possibly better informed by more individualized predictions of the absolute treatment effect. In case of a binary outcome, these predictions of absolute individualized treatment effect require knowledge of the individual's risk without treatment and incorporation of a possibly differential treatment effect (ie, varying with patient characteristics). In this article, we lay out the causal structure of individualized treatment effect in terms of potential outcomes and describe the required assumptions that underlie a causal interpretation of its prediction. Subsequently, we describe regression models and model estimation techniques that can be used to move from average to more individualized treatment effect predictions. We focus mainly on logistic regression-based methods that are both well-known and naturally provide the required probabilistic estimates. We incorporate key components from both causal inference and prediction research to arrive at individualized treatment effect predictions. While the separate components are well known, their successful amalgamation is very much an ongoing field of research. We cut the problem down to its essentials in the setting of a randomized trial, discuss the importance of a clear definition of the estimand of interest, provide insight into the required assumptions, and give guidance with respect to modeling and estimation options. Simulated data illustrate the potential of different modeling options across scenarios that vary both average treatment effect and treatment effect heterogeneity. Two applied examples illustrate individualized treatment effect prediction in randomized trial data.},
year = {2021}
}



@article{hoogland2024,
author = {Hoogland, J. and Efthimiou, O. and Nguyen, T. L. and Debray, T. P. A.},
title = {Evaluating individualized treatment effect predictions: A model-based perspective on discrimination and calibration assessment},
journal = {Statistics in Medicine},
volume = {43},
number = {23},
pages = {4481-4498},
keywords = {treatment effect, individualized, prediction, discrimination, calibration},
doi = {https://doi.org/10.1002/sim.10186},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.10186},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.10186},
abstract = {In recent years, there has been a growing interest in the prediction of individualized treatment effects. While there is a rapidly growing literature on the development of such models, there is little literature on the evaluation of their performance. In this paper, we aim to facilitate the validation of prediction models for individualized treatment effects. The estimands of interest are defined based on the potential outcomes framework, which facilitates a comparison of existing and novel measures. In particular, we examine existing measures of discrimination for benefit (variations of the c-for-benefit), and propose model-based extensions to the treatment effect setting for discrimination and calibration metrics that have a strong basis in outcome risk prediction. The main focus is on randomized trial data with binary endpoints and on models that provide individualized treatment effect predictions and potential outcome predictions. We use simulated data to provide insight into the characteristics of the examined discrimination and calibration statistics under consideration, and further illustrate all methods in a trial of acute ischemic stroke treatment. The results show that the proposed model-based statistics had the best characteristics in terms of bias and accuracy. While resampling methods adjusted for the optimism of performance estimates in the development data, they had a high variance across replications that limited their accuracy. Therefore, individualized treatment effect models are best validated in independent data. To aid implementation, a software implementation of the proposed methods was made available in R.},
year = {2024}
}

@article{willke2012,
  author    = {Richard J. Willke and Zhiyuan Zheng and Prasun Subedi and Rikard Althin and C. Daniel Mullins},
  title     = {From concepts, theory, and evidence of heterogeneity of treatment effects to methodological approaches: a primer},
  journal   = {BMC Medical Research Methodology},
  year      = {2012},
  volume    = {12},
  number    = {1},
  pages     = {185},
  doi       = {10.1186/1471-2288-12-185},
  url       = {https://doi.org/10.1186/1471-2288-12-185},
  issn      = {1471-2288}
}


@article{christensen2021,
title = {Effect Modifiers and Statistical Tests for Interaction in Randomized Trials},
journal = {Journal of Clinical Epidemiology},
volume = {134},
pages = {174-177},
year = {2021},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2021.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0895435621000810},
author = {Robin Christensen and Martijn J.L. Bours and Sabrina M. Nielsen},
abstract = {Statistical analyses of randomized controlled trials (RCTs) yield a causally valid estimate of the overall treatment effect, which is the contrast between the outcomes in two randomized treatment groups commonly accompanied by a confidence interval. In addition, the trial investigators may want to examine whether the observed treatment effect varies across patient subgroups (also called ‘heterogeneity of treatment effects’), i.e. whether the treatment effect is modified by the value of a variable assessed at baseline. The statistical approach for this evaluation of potential effect modifiers is a test for statistical interaction to evaluate whether the treatment effect varies across levels of the effect modifier. In this article, we provide a concise and nontechnical explanation of the use of simple statistical tests for interaction to identify effect modifiers in RCTs. We explain how to calculate the test of interaction by hand, applied to a dataset with simulated data on 1,000 imaginary participants for illustration.}
}

@article{rubin1980,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2287653},
 author = {Donald B. Rubin},
 journal = {Journal of the American Statistical Association},
 number = {371},
 pages = {591--593},
 publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
 title = {Randomization Analysis of Experimental Data: The {Fisher} Randomization Test Comment},
 urldate = {2025-07-05},
 volume = {75},
 year = {1980}
}


@article{rosenbaum1983,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2335942},
 abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two-dimensional plot.},
 author = {Paul R. Rosenbaum and Donald B. Rubin},
 journal = {Biometrika},
 number = {1},
 pages = {41--55},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Central Role of the Propensity Score in Observational Studies for Causal Effects},
 urldate = {2025-07-05},
 volume = {70},
 year = {1983}
}

@article{kunzel2019,
author = {S{\"o}ren R. K{\"u}nzel  and Jasjeet S. Sekhon  and Peter J. Bickel  and Bin Yu },
title = {Metalearners for estimating heterogeneous treatment effects using machine learning},
journal = {Proceedings of the National Academy of Sciences},
volume = {116},
number = {10},
pages = {4156-4165},
year = {2019},
doi = {10.1073/pnas.1804597116},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1804597116},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1804597116},
abstract = {Estimating and analyzing heterogeneous treatment effects is timely, yet challenging. We introduce a unifying framework for many conditional average treatment effect estimators, and we propose a metalearner, the X-learner, which can adapt to structural properties, such as the smoothness and sparsity of the underlying treatment effect. We present its favorable properties, using theory and simulations. We apply it, using random forests, to two field experiments in political science, where it is shown to be easy to use and to produce results that are interpretable. There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms-such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks-to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.}}


@article{herzog2023,
author = {Herzog, Lisa and Kook, Lucas and G{\"o}tschi, Andrea and Petermann, Katrin and H{\"a}nsel, Martin and Hamann, Janne and D{\"u}rr, Oliver and Wegener, Susanne and Sick, Beate},
title = {Deep transformation models for functional outcome prediction after acute ischemic stroke},
journal = {Biometrical Journal},
volume = {65},
number = {6},
pages = {2100379},
keywords = {deep learning, distributional regression, ordinal regression, transformation models},
doi = {https://doi.org/10.1002/bimj.202100379},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.202100379},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.202100379},
abstract = {Abstract In many medical applications, interpretable models with high prediction performance are sought. Often, those models are required to handle semistructured data like tabular and image data. We show how to apply deep transformation models (DTMs) for distributional regression that fulfill these requirements. DTMs allow the data analyst to specify (deep) neural networks for different input modalities making them applicable to various research questions. Like statistical models, DTMs can provide interpretable effect estimates while achieving the state-of-the-art prediction performance of deep neural networks. In addition, the construction of ensembles of DTMs that retain model structure and interpretability allows quantifying epistemic and aleatoric uncertainty. In this study, we compare several DTMs, including baseline-adjusted models, trained on a semistructured data set of 407 stroke patients with the aim to predict ordinal functional outcome three months after stroke. We follow statistical principles of model-building to achieve an adequate trade-off between interpretability and flexibility while assessing the relative importance of the involved data modalities. We evaluate the models for an ordinal and dichotomized version of the outcome as used in clinical practice. We show that both tabular clinical and brain imaging data are useful for functional outcome prediction, whereas models based on tabular data only outperform those based on imaging data only. There is no substantial evidence for improved prediction when combining both data modalities. Overall, we highlight that DTMs provide a powerful, interpretable approach to analyzing semistructured data and that they have the potential to support clinical decision-making.},
year = {2023}
}

@inproceedings{kingma2015,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P. and Ba, Jimmy},
  booktitle={Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
  year={2015},
  url={https://arxiv.org/abs/1412.6980}
}

@article{hothorn2018,
author = {Hothorn, Torsten and M{\"o}st, Lisa and B{\"u}hlmann, Peter},
title = {Most Likely Transformations},
journal = {Scandinavian Journal of Statistics},
volume = {45},
number = {1},
pages = {110-134},
keywords = {censoring, conditional distribution function, conditional quantile function, distribution regression, transformation model, truncation},
doi = {https://doi.org/10.1111/sjos.12291},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/sjos.12291},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/sjos.12291},
abstract = {Abstract We propose and study properties of maximum likelihood estimators in the class of conditional transformation models. Based on a suitable explicit parameterization of the unconditional or conditional transformation function, we establish a cascade of increasingly complex transformation models that can be estimated, compared and analysed in the maximum likelihood framework. Models for the unconditional or conditional distribution function of any univariate response variable can be set up and estimated in the same theoretical and computational framework simply by choosing an appropriate transformation function and parameterization thereof. The ability to evaluate the distribution function directly allows us to estimate models based on the exact likelihood, especially in the presence of random censoring or truncation. For discrete and continuous responses, we establish the asymptotic normality of the proposed estimators. A reference software implementation of maximum likelihood-based estimation for conditional transformation models that allows the same flexibility as the theory developed here was employed to illustrate the wide range of possible applications.},
year = {2018}
}

@article{pearl1995,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2337329},
 abstract = {The primary aim of this paper is to show how graphical models can be used as a mathematical language for integrating statistical and subject-matter information. In particular, the paper develops a principled, nonparametric framework for causal inference, in which diagrams are queried to determine if the assumptions available are sufficient for identifying causal effects from nonexperimental data. If so the diagrams can be queried to produce mathematical expressions for causal effects in terms of observed distributions; otherwise, the diagrams can be queried to suggest additional observations or auxiliary experiments from which the desired inferences can be obtained.},
 author = {Judea Pearl},
 journal = {Biometrika},
 number = {4},
 pages = {669--688},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Causal Diagrams for Empirical Research},
 urldate = {2025-05-31},
 volume = {82},
 year = {1995}
}





@misc{zheng2018,
      title={{DAGs} with NO TEARS: Continuous Optimization for Structure Learning}, 
      author={Xun Zheng and Bryon Aragam and Pradeep Ravikumar and Eric P. Xing},
      year={2018},
      eprint={1803.01422},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
  note         = {arXiv preprint 1803.01422, accepted at NeurIPS 2018},
      url={https://arxiv.org/abs/1803.01422}, 
}


@article{rubin2005,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/27590541},
 abstract = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism - a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials.},
 author = {Donald B. Rubin},
 journal = {Journal of the American Statistical Association},
 number = {469},
 pages = {322--331},
 publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
 title = {Causal Inference Using Potential Outcomes: Design, Modeling, Decisions},
 urldate = {2025-07-05},
 volume = {100},
 year = {2005}
}


@article{holland1986,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2289064},
 abstract = {Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation, and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about causation? This question is addressed by using a particular model for causal inference (Holland and Rubin 1983; Rubin 1974) to critique the discussions of other writers on causation and causal inference. These include selected philosophers, medical researchers, statisticians, econometricians, and proponents of causal modeling.},
 author = {Paul W. Holland},
 journal = {Journal of the American Statistical Association},
 number = {396},
 pages = {945--960},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Statistics and Causal Inference},
 urldate = {2025-07-05},
 volume = {81},
 year = {1986}
}


@article{curth2024,
author = {Curth, Alicia and Peck, Richard W. and McKinney, Eoin and Weatherall, James and van der Schaar, Mihaela},
title = {Using Machine Learning to Individualize Treatment Effect Estimation: Challenges and Opportunities},
journal = {Clinical Pharmacology \& Therapeutics},
volume = {115},
number = {4},
pages = {710-719},
doi = {https://doi.org/10.1002/cpt.3159},
url = {https://ascpt.onlinelibrary.wiley.com/doi/abs/10.1002/cpt.3159},
eprint = {https://ascpt.onlinelibrary.wiley.com/doi/pdf/10.1002/cpt.3159},
abstract = {The use of data from randomized clinical trials to justify treatment decisions for real-world patients is the current state of the art. It relies on the assumption that average treatment effects from the trial can be extrapolated to patients with personal and/or disease characteristics different from those treated in the trial. Yet, because of heterogeneity of treatment effects between patients and between the trial population and real-world patients, this assumption may not be correct for many patients. Using machine learning to estimate the expected conditional average treatment effect (CATE) in individual patients from observational data offers the potential for more accurate estimation of the expected treatment effects in each patient based on their observed characteristics. In this review, we discuss some of the challenges and opportunities for machine learning to estimate CATE, including ensuring identification assumptions are met, managing covariate shift, and learning without access to the true label of interest. We also discuss the potential applications as well as future work and collaborations needed to further improve identification and utilization of CATE estimates to increase patient benefit.},
year = {2024}
}



@inproceedings{reisach2021,
author = {Reisach, Alexander G. and Seiler, Christof and Weichwald, Sebastian},
title = {Beware of the simulated {DAG}! {C}ausal discovery benchmarks may be easy to game},
year = {2021},
isbn = {9781713845393},
abstract = {Simulated DAG models may exhibit properties that, perhaps inadvertently, render their structure identifiable and unexpectedly affect structure learning algorithms. Here, we show that marginal variance tends to increase along the causal order for generically sampled additive noise models. We introduce varsortability as a measure of the agreement between the order of increasing marginal variance and the causal order. For commonly sampled graphs and model parameters, we show that the remarkable performance of some continuous structure learning algorithms can be explained by high varsortability and matched by a simple baseline method. Yet, this performance may not transfer to real-world data where varsortability may be moderate or dependent on the choice of measurement scales. On standardized data, the same algorithms fail to identify the ground-truth DAG or its Markov equivalence class. While standardization removes the pattern in marginal variance, we show that data generating processes that incur high varsortability also leave a distinct covariance pattern that may be exploited even after standardization. Our findings challenge the significance of generic benchmarks with independently drawn parameters.},
booktitle = {Proceedings of the 35th Conference on Neural Information Processing Systems (NeurIPS)},
articleno = {2127},
numpages = {13}
}

@article{friedman2010,
 title={Regularization Paths for Generalized Linear Models via Coordinate Descent},
 volume={33},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v033i01},
 doi={10.18637/jss.v033.i01},
 number={1},
 journal={Journal of Statistical Software},
 author={Friedman, Jerome H. and Hastie, Trevor and Tibshirani, Rob},
 year={2010},
 pages={1-22}
}

@article{breiman2001,
author = {Breiman, Leo},
title = {{Random Forests}},
year = {2001},
issue_date = {October 1 2001},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1010933404324},
doi = {10.1023/A:1010933404324},
journal = {Machine Learning},
month = oct,
pages = {5-32},
numpages = {28},
keywords = {classification, ensemble, regression}
}

@article{calster2020,
author = {Ben Van Calster and Maarten van Smeden and Bavo De Cock and Ewout W Steyerberg},
title ={Regression shrinkage methods for clinical prediction models do not guarantee improved performance: Simulation study},

journal = {Statistical Methods in Medical Research},
volume = {29},
number = {11},
pages = {3166-3178},
year = {2020},
doi = {10.1177/0962280220921415},
    note ={PMID: 32401702},

URL = { https://doi.org/10.1177/0962280220921415},
eprint = { 
        https://doi.org/10.1177/0962280220921415
}
,
    abstract = { When developing risk prediction models on datasets with limited sample size, shrinkage methods are recommended. Earlier studies showed that shrinkage results in better predictive performance on average. This simulation study aimed to investigate the variability of regression shrinkage on predictive performance for a binary outcome. We compared standard maximum likelihood with the following shrinkage methods: uniform shrinkage (likelihood-based and bootstrap-based), penalized maximum likelihood (ridge) methods, LASSO logistic regression, adaptive LASSO, and Firth’s correction. In the simulation study, we varied the number of predictors and their strength, the correlation between predictors, the event rate of the outcome, and the events per variable. In terms of results, we focused on the calibration slope. The slope indicates whether risk predictions are too extreme (slope < 1) or not extreme enough (slope > 1). The results can be summarized into three main findings. First, shrinkage improved calibration slopes on average. Second, the between-sample variability of calibration slopes was often increased relative to maximum likelihood. In contrast to other shrinkage approaches, Firth’s correction had a small shrinkage effect but showed low variability. Third, the correlation between the estimated shrinkage and the optimal shrinkage to remove overfitting was typically negative, with Firth’s correction as the exception. We conclude that, despite improved performance on average, shrinkage often worked poorly in individual datasets, in particular when it was most needed. The results imply that shrinkage methods do not solve problems associated with small sample size or low number of events per variable. }
}


@article{riley2021,
title = {Penalization and shrinkage methods produced unreliable clinical prediction models especially when sample size was small},
journal = {Journal of Clinical Epidemiology},
volume = {132},
pages = {88-96},
year = {2021},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2020.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0895435620312099},
author = {Richard D. Riley and Kym I.E. Snell and Glen P. Martin and Rebecca Whittle and Lucinda Archer and Matthew Sperrin and Gary S. Collins},
keywords = {Risk prediction models, Penalization, Shrinkage, Overfitting, Sample size},
abstract = {Objectives
When developing a clinical prediction model, penalization techniques are recommended to address overfitting, as they shrink predictor effect estimates toward the null and reduce mean-square prediction error in new individuals. However, shrinkage and penalty terms (‘tuning parameters’) are estimated with uncertainty from the development data set. We examined the magnitude of this uncertainty and the subsequent impact on prediction model performance.
Study Design and Setting
This study comprises applied examples and a simulation study of the following methods: uniform shrinkage (estimated via a closed-form solution or bootstrapping), ridge regression, the lasso, and elastic net.
Results
In a particular model development data set, penalization methods can be unreliable because tuning parameters are estimated with large uncertainty. This is of most concern when development data sets have a small effective sample size and the model's Cox-Snell R2 is low. The problem can lead to considerable miscalibration of model predictions in new individuals.
Conclusion
Penalization methods are not a ‘carte blanche’; they do not guarantee a reliable prediction model is developed. They are more unreliable when needed most (i.e., when overfitting may be large). We recommend they are best applied with large effective sample sizes, as identified from recent sample size calculations that aim to minimize the potential for model overfitting and precisely estimate key parameters.}
}


@inproceedings{guo2017,
  author       = {Chuan Guo and
                  Geoff Pleiss and
                  Yu Sun and
                  Kilian Q. Weinberger},
  title        = {On Calibration of Modern Neural Networks},
booktitle    = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
  volume       = {70},
  pages        = {1321--1330},
publisher = {Proceedings of Machine Learning Research},
  year         = {2017},
  url          = {http://proceedings.mlr.press/v70/guo17a.html},
  timestamp    = {Wed, 02 Oct 2024 14:40:04 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/GuoPSW17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hartford2017,
author = {Hartford, Jason and Lewis, Greg and Leyton-Brown, Kevin and Taddy, Matt},
title = {Deep IV: a flexible approach for counterfactual prediction},
year = {2017},
publisher = {JMLR.org},
abstract = {Counterfactual prediction requires understanding causal relationships between so-called treatment and outcome variables. This paper provides a recipe for augmenting deep learning methods to accurately characterize such relationships in the presence of instrument variables (IVs)-sources of treatment randomization that are conditionally independent from the outcomes. Our IV specification resolves into two prediction tasks that can be solved with deep neural nets: a first-stage network for treatment prediction and a second-stage network whose loss function involves integration over the conditional treatment distribution. This Deep IV framework allows us to take advantage of off-the-shelf supervised learning techniques to estimate causal effects by adapting the loss function. Experiments show that it outperforms existing machine learning approaches.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning (ICML)},
volume = {70},
pages = {1414--1423},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}



@misc{frauen2023,
      title={Estimating individual treatment effects under unobserved confounding using binary instruments}, 
      author={Dennis Frauen and Stefan Feuerriegel},
      note  = {Accepted at ICLR 2023},
      year={2023},
      eprint={2208.08544},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2208.08544}, 
}


@article{IST1997,
title = {The {International Stroke Trial (IST)}: a randomised trial of aspirin, subcutaneous heparin, both, or neither among 19,435 patients with acute ischaemic stroke},
author = {{International Stroke Trial Collaborative Group}},
journal = {The Lancet},
volume = {349},
number = {9065},
pages = {1569-1581},
year = {1997},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(97)04011-7},
url = {https://doi.org/10.1016/S0140-6736(97)04011-7}
}

@article{sandercock2011,
  author    = {Sandercock, Peter A.G. and Niewada, Maciej and Członkowska, Anna and the International Stroke Trial Collaborative Group},
  title     = {{The International Stroke Trial database}},
  journal   = {Trials},
  year      = {2011},
  volume    = {12},
  number    = {1},
  pages     = {101},
  doi       = {10.1186/1745-6215-12-101},
  url       = {https://doi.org/10.1186/1745-6215-12-101},
  issn      = {1745-6215}
}

@misc{vegetabile2021,
      title={On the Distinction Between "Conditional Average Treatment Effects" ({CATE}) and "Individual Treatment Effects" ({ITE}) Under Ignorability Assumptions}, 
      author={Brian G. Vegetabile},
      year={2021},
      eprint={2108.04939},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      note         = {arXiv preprint 2108.04939, presented at the Workshop on the Neglected Assumptions in Causal Inference (NACI), 38th International Conference on Machine Learning, 2021},
    url={https://arxiv.org/abs/2108.04939}, 
}


@article{chernozhukov2005,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/3598944},
 abstract = {The ability of quantile regression models to characterize the heterogeneous impact of variables on different points of an outcome distribution makes them appealing in many economic applications. However, in observational studies, the variables of interest (e.g., education, prices) are often endogenous, making conventional quantile regression inconsistent and hence inappropriate for recovering the causal effects of these variables on the quantiles of economic outcomes. In order to address this problem, we develop a model of quantile treatment effects (QTE) in the presence of endogeneity and obtain conditions for identification of the QTE without functional form assumptions. The principal feature of the model is the imposition of conditions that restrict the evolution of ranks across treatment states. This feature allows us to overcome the endogeneity problem and recover the true QTE through the use of instrumental variables. The proposed model can also be equivalently viewed as a structural simultaneous equation model with nonadditive errors, where QTE can be interpreted as the structural quantile effects (SQE).},
 author = {Victor Chernozhukov and Christian Hansen},
 journal = {Econometrica},
 number = {1},
 pages = {245--261},
 publisher = {[Wiley, Econometric Society]},
 title = {An {IV} Model of Quantile Treatment Effects},
 urldate = {2025-07-06},
 volume = {73},
 year = {2005}
}

@misc{dandl2025,
      title={Nonparanormal Adjusted Marginal Inference}, 
      author={Susanne Dandl and Torsten Hothorn},
      year={2025},
      eprint={2503.01657},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2503.01657}, 
  note         = {arXiv preprint 2503.01657}
}

@article{liu2009,
author = {Liu, Han and Lafferty, John and Wasserman, Larry},
title = {The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
month = dec,
pages = {2295-2328},
numpages = {34}
}


@article{klein2022,
author = {Klein, Nadja and Hothorn, Torsten and Barbanti, Luisa and Kneib, Thomas},
title = {Multivariate conditional transformation models},
journal = {Scandinavian Journal of Statistics},
volume = {49},
number = {1},
pages = {116-142},
keywords = {constrained optimization, copula, marginal distributions, most likely transformations, multivariate regression, normalizing flows, seemingly unrelated regression},
doi = {https://doi.org/10.1111/sjos.12501},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/sjos.12501},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/sjos.12501},
abstract = {Abstract Regression models describing the joint distribution of multivariate responses conditional on covariate information have become an important aspect of contemporary regression analysis. However, a limitation of such models are the rather simplistic assumptions often made, for example, a constant dependence structure not varying with covariates or the restriction to linear dependence between the responses. We propose a general framework for multivariate conditional transformation models that overcomes these limitations and describes the entire distribution in a tractable and interpretable yet flexible way conditional on nonlinear effects of covariates. The framework can be embedded into likelihood-based inference, including results on asymptotic normality, and allows the dependence structure to vary with covariates. In addition, it scales well-beyond bivariate response situations, which were the main focus of most earlier investigations. We illustrate the benefits in a trivariate analysis of childhood undernutrition and demonstrate empirically that complex truly multivariate data-generating processes can be inferred from observations.},
year = {2022}
}

@article{aalen2015,
author = {Aalen, Odd and Cook, Richard and R{\o}ysland, Kjetil},
year = {2015},
month = {06},
pages = {579--593},
title = {Does {Cox} analysis of a randomized survival study yield a causal treatment effect?},
volume = {21},
journal = {Lifetime Data Analysis},
doi = {10.1007/s10985-015-9335-y}
}

@Manual{comets,
  title        = {comets: Covariance Measure Tests for Conditional Independence},
  author       = {Lucas Kook},
  year         = {2024},
  note         = {R package version 0.1-1},
  url          = {https://github.com/LucasKook/comets},
  urldate      = {2025-07-08}
}


@Manual{reticulate,
  title = {reticulate: Interface to 'Python'},
  author = {Kevin Ushey and JJ Allaire and Yuan Tang},
  year = {2025},
  note = {R package version 1.42.0},
  url = {https://rstudio.github.io/reticulate/},
}

@misc{keras,
  title={{R Interface to 'Keras'}},
  author={Chollet, Fran\c{c}ois and Allaire, JJ and others},
  year={2017},
  publisher={GitHub},
  note = {R package version 2.15.0},
  url = {https://github.com/rstudio/keras}
}
 %howpublished={\url{https://github.com/rstudio/keras}},

@Manual{tensorflow,
    title = {tensorflow: R Interface to 'TensorFlow'},
    author = {JJ Allaire and Yuan Tang},
    year = {2025},
    note = {R package version 2.16.9},
    url = {https://github.com/rstudio/tensorflow},
  }

@Manual{R,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2024},
    url = {https://www.R-project.org/},
  }


@article{srivastava2014,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: A simple way to prevent neural networks from overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {Journal of Machine Learning Research},
month = jan,
pages = {1929--1958},
numpages = {30},
keywords = {regularization, neural networks, model combination, deep learning}
}


@incollection{prechelt2012,
author="Prechelt, Lutz",
editor="Orr, Genevi{\`e}ve B.
and M{\"u}ller, Klaus-Robert",
title="Early Stopping --- But When?",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer",
address="Berlin, Heidelberg",
pages="53--67",
abstract="Validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (``early stopping''). The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeoff between training time and generalization: From the given mix of 1296 training runs using different 12 problems and 24 different network architectures I conclude slower stopping criteria allow for small improvements in generalization (here: about 4{\%} on average), but cost much more training time (here: about factor 4 longer on average).",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_5",
url="https://doi.org/10.1007/978-3-642-35289-8_5"
}


@inproceedings{Ioffe2015,
author = {Ioffe, Sergey and Szegedy, Christian},
title = {Batch {N}ormalization: Accelerating deep network training by reducing internal covariate shift},
year = {2015},
publisher = {Proceedings of Machine Learning Research},
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
volume = {37},
pages = {448--456},
numpages = {9},
location = {Lille, France}
}


@inproceedings{glorot2011,
  title={Deep Sparse Rectifier Neural Networks},
  author={Xavier Glorot and Antoine Bordes and Yoshua Bengio},
  booktitle={Proceedings of the 14th International Conference on Artificial Intelligence and Statistics},
  year={2011},
  url={https://api.semanticscholar.org/CorpusID:2239473},
publisher = {Proceedings of Machine Learning Research},
volume = {15},
pages = {315--323}
}


@article{rumelhart1986,
  title={Learning representations by back-propagating errors},
  author={David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986},
  volume={323},
  pages={533-536},
  url={https://api.semanticscholar.org/CorpusID:205001834}
}


@article{schmidhuber2015,
title = {Deep learning in neural networks: An overview},
journal = {Neural Networks},
volume = {61},
pages = {85-117},
year = {2015},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0893608014002135},
author = {J{\"u}rgen Schmidhuber},
keywords = {Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.}
}


@inproceedings{papamakarios2017,
 author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems (NeurIPS},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {2335--2344},
numpages = {10},
 publisher = {Curran Associates, Inc.},
 title = {Masked Autoregressive Flow for Density Estimation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf},
 volume = {30},
 year = {2017}
}
